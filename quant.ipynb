{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpYuHrvISc31YFzN0FiSac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emegnahc/quant/blob/main/quant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On dw llama.cpp"
      ],
      "metadata": {
        "id": "797rz77qJgYR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66o5LACNJNiz"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ggml-org/llama.cpp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On le build"
      ],
      "metadata": {
        "id": "wWN9Y3eJJ0VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd llama.cpp && cmake -B build -DGGML_CUDA=ON && cmake --build build --config Release"
      ],
      "metadata": {
        "id": "G-VUmDa8Je3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install des requirements / peut-être un redem de kernel et une réinstall du repo llama.cpp..."
      ],
      "metadata": {
        "id": "0rhmDXQfKfKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r llama.cpp/requirements.txt"
      ],
      "metadata": {
        "id": "NAOgcYD8J-91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On choisi le modèle qui va bien. Si ressource suffisante sinon, un plus petit de la même famille pour rester au plus prêt"
      ],
      "metadata": {
        "id": "fbuFJxV1KNGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"Qwen/Qwen3-Coder-480B-A35B-Instruct\"\n",
        "MODEL_NAME = MODEL_ID.split(\"/\")[-1]"
      ],
      "metadata": {
        "id": "pcIcA5ZfKT4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On récup le modèle depuis HF"
      ],
      "metadata": {
        "id": "9zaCCA09LR3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/{MODEL_ID}"
      ],
      "metadata": {
        "id": "w3kSBc4JLZoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A essayer en convertissant d'abord en fp16 voir en skippant ce step...\n",
        "Voir si possible et si résultat différent"
      ],
      "metadata": {
        "id": "bWYJs4GjLbow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "!python llama.cpp/examples/convert_legacy_llama.py {MODEL_NAME} --outtype f16 --outfile {fp16}"
      ],
      "metadata": {
        "id": "1cmsFNb2LmIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée la matrice d'importance qui n'est pas une page mais qui s'obtient en inférent sur le dataset de \"calibration\".\n",
        "A retrouver la syntaxe exacte sur le repo llama.cpp\n"
      ],
      "metadata": {
        "id": "NIK6RkLDL4-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama.cpp/build/bin/llama-imatrix"
      ],
      "metadata": {
        "id": "ZPIHoK7VMXAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On choisi de la quantif 4bits étant donné ce que l'on peut attendre des perfs cf. certains tests réalisés par des utilisateurs.\n",
        "Il faut ajouter le fichier créé à l'étape précédente"
      ],
      "metadata": {
        "id": "Og-XITWkMXm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "method = \"IQ4_XS\"\n",
        "qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{METHOD}.gguf\"\n",
        "!./llama.cpp/build/bin/llama-quantize {fp16} {qtype} {METHOD} --imatrix TO_BE_COMPLETED"
      ],
      "metadata": {
        "id": "wmIQwwdyMKvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite on évalue avec llama-perplexity. A compléter la cellule ci-dessous cf.doc sur le repo llama.cpp avec les options pour avoir le plus de détails"
      ],
      "metadata": {
        "id": "TR8YWcjYPECW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama.cpp/build/bin/llama-perplexity"
      ],
      "metadata": {
        "id": "lx46ZsIWPMD4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}